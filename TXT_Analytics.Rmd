---
title: "Text Analytics"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Data Pub: 11-12-2017
# Autor: André Placido
# Objetivo: Captura de tweets e uso de técnicas do Text Analytics utilizando R
```


## R Text Anaytics com fonte em dados do Twitter

As redes sociais são uma importante fonte de informações para qualquer empresa, pessoa, negócio, serviço ou produto. Por essa relevância utilizarei o Twitter como fonte, sendo que o foco do processo será determinar para um período o relacionamento desses tweets, frequencia, ou seja, aplicar técnicas de análise textual e ver se conseguimos resultados interessantes.

### Contexto da captura dos dados

O foco do projeto não é o detalhamento de um códico de captura de dados do Twitter, então me limitarei a explicar as condições ou contexto de captura dos dados. 

Foi utilizado o timeline do usuário G1
Data: 27-nov-2017 as 16:16:00
Limite de até 3200 tweets
capturada as informações os dados foram gravados em um arquivo "XLS" e disponibilizado no repositório no GitHub e pode ser ser encontrato nesse link <https://github.com/andreplacido/Text-Analytics-Twitter>

### Explorando o dataset

Com posse dessas informações, vamos carregar o dataset e visualizar uma amostra desses dados.

```{r }
library(rJava)
library(xlsx)

df<-read.xlsx("D:/Td2i/GitHub/Text-Analytics-Twitter/dataset/tl2.g1.xlsx", 1)
```

```{r}
# visualizando apenas as 10 linhas das mensagens do Twitter

head(df$text, 10)


```



### Limpeza dos dados

Com a amostra dos dados já percebemos que teremos alguns tratamentos que precisam ser feitos para uma análise com qualidade. 
COmo trata-se de uma fonte sem restrições ou corretores, os dados oriundos das redes sociais e especialmente do Twitter podemos nos defrontar com diversas situações, detre elas: acentuação ou falta, palavras escritas de forma incorreta ou até mesmo ter um volume de uma determinada palavra que não é relevante, como por exemplo os artigos e preposições. 
Com esse cenário, se faz necessário uma série de processamentos ou tratamentos chamados de limpeza de dados ou data wrangling.

 
```{r}
#Retirando acentuacao das palavras
library(stringi)
dados_str<-stri_trans_general(df$text, "Latin-ASCII")
```

### Text Mining do R (tm)

Para o R temos um pacote chamado "tm" ou Text Mining in R. Essa biblioteca ou pacote possui funcionalidades que muito ajudam no processo de Limpeza para Texto. Você pode aprofundar seus conhecimento desse pacote acessando o seguinte link <https://www.rdocumentation.org/packages/tm/versions/0.7-2>

```{r}
library(tm)
myCorpus <- VCorpus(VectorSource(dados_str))

myCorpus <- tm_map(myCorpus, content_transformer(tolower) )
myCorpus <- tm_map(myCorpus, function(x) gsub('http[[:alnum:]]*','',x)) #removendo URLs
myCorpus <- tm_map(myCorpus, function(x) gsub("[^[:alpha:][:space:]]*", "", x))
myCorpus <- tm_map(myCorpus, removeNumbers) #removendo números
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("portuguese")))
# incluindo minhas stopwords
myStopwords <- c("sao","se","no","vai","pode","aspas", "rockinrio", "que", "ainda", "pra", "ficar" , "porque", "faz", "como","tcoedfdfwtyt")
myCorpus <- tm_map(myCorpus,removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
```

## Visualização
Antes de iniciar os plots ainda são necessárias algumas preparações.

Matriz para a contagem de termos
Vamos criar uma matriz que considere apenas as palvras com uma frequencia superior ou igual a 30. Ou seja, que foram mensionadas nos Tweets pelo menos 30 vezes.

```{r}

corpus_text <- tm_map(myCorpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus_text, control = list(minWordLength = 1 ))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >=30) #filtra a quantidade de palavras pela qtd de frequencia

df <- data.frame(term = names(term.freq), freq = term.freq)
```

### Visualização Gráfico de frequencia


```{r}
library(ggplot2)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Count") + coord_flip() +
  theme(axis.text=element_text(size=7))
```

### Word CLoud

```{r}
library(wordcloud)
m <- as.matrix(tdm)

# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]


# plot word cloud

wordcloud(words = names(word.freq), freq = word.freq, min.freq = 25,
          random.order = F, colors = pal)
```

### Network of Terms

```{r}

freq.term <-findFreqTerms(tdm, lowfreq = 50) 

library(graph)
library(Rgraphviz)
plot(tdm, term = freq.term, corThreshold = 0.1)
```


